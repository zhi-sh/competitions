# -*- coding: utf-8 -*-
# @DateTime :2020/12/23 下午9:15
# @Author   :zhi.liu

# ------------------------------------------------------------------------------
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable


# 1. Focal Loss 处理数据不平衡问题
class MultiFocalLoss(nn.Module):
    def __init__(self, num_class, alpha=None, gamma=2, blance_index=-1, smooth=None, size_average=True):
        super(MultiFocalLoss, self).__init__()
        self.num_class = num_class  # 类别数目
        self.alpha = alpha  # 平滑因子
        self.gamma = gamma  # 指数
        self.smooth = smooth  # 标签平滑
        self.size_average = size_average  # 返回时是否需要mean一下

        # 初始化
        if self.alpha is None:
            self.alpha = torch.ones(self.num_class, 1)
        elif isinstance(self.alpha, (list, np.ndarray)):
            assert len(self.alpha) == self.num_class
            self.alpha = torch.FloatTensor(alpha).view(self.num_class, -1)
            self.alpha = self.alpha / self.alpha.sum()
        elif isinstance(self.alpha, float):
            self.alpha = torch.ones(self.num_class, 1)
            alpha = alpha * (1 - self.alpha)
            alpha[blance_index] = self.alpha
            self.alpha = alpha
        else:
            raise ValueError('Not support alpha type.')

        if self.smooth is not None:
            if self.smooth < 0 or self.smooth > 1.0:
                raise ValueError('Smooth value should be in (0,1)')

    def forward(self, input, target):
        logit = F.softmax(input, dim=1)

        if logit.dim() > 2:
            logit = logit.view(logit.size(0), logit.size(1), -1)
            logit = logit.permute(0, 2, 1).contiguous()
            logit = logit.view(-1, logit.size(-1))
        target = target.view(-1, 1)

        epsilon = 1e-10
        alpha = self.alpha
        if alpha.device != input.device:
            alpha = alpha.to(input.device)

        idx = target.cpu().long()
        one_hot_key = torch.FloatTensor(target.size(0), self.num_class).zero_()
        one_hot_key = one_hot_key.scatter_(1, idx, 1)
        if one_hot_key.device != logit.device:
            one_hot_key = one_hot_key.to(logit.device)

        if self.smooth:
            one_hot_key = torch.clamp(one_hot_key, self.smooth, 1.0 - self.smooth)
        pt = (one_hot_key * logit).sum(1) + epsilon
        logpt = pt.log()

        gamma = self.gamma
        alpha = alpha[idx]
        loss = -1 * alpha * torch.pow((1 - pt), gamma) * logpt

        if self.size_average:
            loss = loss.mean()
        else:
            loss = loss.sum()

        return loss


class LBTW(nn.Module):
    def __init__(self, alpha=0.5, num_tasks=3):
        super(LBTW, self).__init__()
        self.init_loss = torch.ones(num_tasks)
        self.alpha = alpha

    def forward(self, *losses, batch_idx=0):
        batch_loss = torch.tensor(losses, device=losses[0].device)
        if batch_idx == 0:
            self.init_loss = batch_loss.detach()
        weights = torch.pow(self.init_loss / batch_loss, self.alpha).detach()
        weights /= torch.mean(weights)
        weights *= 3

        return losses[0] * weights[0], losses[1] * weights[1], losses[2] * weights[2]


# 2. 对抗训练
class FGM:
    def __init__(self, model):
        self.model = model
        self.backup = {}

    def attack(self, epsilon=1., emb_name='word_embeddings'):
        # emb_name为模型中embedding的参数名
        for name, param in self.model.named_parameters():
            if param.requires_grad and emb_name in name:
                self.backup[name] = param.data.clone()
                norm = torch.norm(param.grad)
                if norm != 0:
                    r_at = epsilon * param.grad / norm
                    param.data.add_(r_at)

    def restore(self, emb_name='word_embeddings'):
        # emb_name为模型中embedding的参数名
        for name, param in self.model.named_parameters():
            if param.requires_grad and emb_name in name:
                assert name in self.backup
                param.data = self.backup[name]
        self.backup = {}
